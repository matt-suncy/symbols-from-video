{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'code.lib'; 'code' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/Documents/latplan-temporal-segmentation/scripts/pretrained_model_experiments/GACLIP_chin_chess.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://lobot.caslab.queensu.ca/home/jovyan/Documents/latplan-temporal-segmentation/scripts/pretrained_model_experiments/GACLIP_chin_chess.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m../\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://lobot.caslab.queensu.ca/home/jovyan/Documents/latplan-temporal-segmentation/scripts/pretrained_model_experiments/GACLIP_chin_chess.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mGACLIP_submodule\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcode\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m load_model_weights,mkdir_p\n\u001b[0;32m---> <a href='vscode-notebook-cell://lobot.caslab.queensu.ca/home/jovyan/Documents/latplan-temporal-segmentation/scripts/pretrained_model_experiments/GACLIP_chin_chess.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mGACLIP_submodule\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcode\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mGALIP\u001b[39;00m \u001b[39mimport\u001b[39;00m NetG, CLIP_TXT_ENCODER\n",
      "File \u001b[0;32m~/Documents/latplan-temporal-segmentation/src/GACLIP_submodule/code/models/GALIP.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m OrderedDict\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m dummy_context_mgr\n\u001b[1;32m     10\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCLIP_IMG_ENCODER\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m     11\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, CLIP):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'code.lib'; 'code' is not a package"
     ]
    }
   ],
   "source": [
    "# Documents/latplan-temporal-segmentation\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import clip\n",
    "import os.path as osp\n",
    "import os, sys\n",
    "import torchvision.utils as vutils\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from src.GACLIP_submodule.code.lib.utils import load_model_weights,mkdir_p\n",
    "from src.GACLIP_submodule.code.models.GALIP import NetG, CLIP_TXT_ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' # 'cpu' # 'cuda:0'\n",
    "CLIP_text = \"ViT-B/32\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_model = clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = CLIP_TXT_ENCODER(clip_model).to(device)\n",
    "netG = NetG(64, 100, 512, 256, 3, False, clip_model).to(device)\n",
    "path = '../saved_models/pretrained/pre_cc12m.pth'\n",
    "checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "netG = load_model_weights(netG, checkpoint['model']['netG'], multi_gpus=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "noise = torch.randn((batch_size, 100)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = ['Line drawing illustration of a kawaii cute ghost.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from text\n",
    "with torch.no_grad():\n",
    "    for i in range(len(captions)):\n",
    "        caption = captions[i]\n",
    "        tokenized_text = clip.tokenize([caption]).to(device)\n",
    "        sent_emb, word_emb = text_encoder(tokenized_text)\n",
    "        sent_emb = sent_emb.repeat(batch_size,1)\n",
    "        fake_imgs = netG(noise,sent_emb,eval=True).float()\n",
    "        name = f'{captions[i].replace(\" \", \"-\")}'\n",
    "        vutils.save_image(fake_imgs.data, '../samples/%s.png'%(name), nrow=8, value_range=(-1, 1), normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "849434eb86c3997df801551b732438d01b491303b69c29efcf332721ce6d8430"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
